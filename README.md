# Data Engineering Project - Stock Analysis by the minute üìàüìäüêÇ

The Data Engineering Project is a data pipeline built on Google Cloud Platform (GCP) that retrieves real-time stock data from the Metatrader 5 platform and identifies the stocks with the highest positive or negative variation in the last minute. This project aims to provide valuable insights into stock market trends and facilitate informed decision-making for traders and investors, focusing on identifying potential trades with a good risk-reward ratio.

## Problem Statement

Traditional stock market analysis tools, such as Google Sheets, often suffer from a significant time delay in receiving real-time stock data. In the case of Google Sheets, this delay can be as long as 15 minutes, which hampers timely decision-making in fast-paced markets. Traders and investors require access to up-to-date information to make informed choices and take advantage of rapidly changing market conditions.

## Solution

The Data Engineering Project addresses the time delay problem by utilizing the power of Metatrader 5, a popular trading platform known for its real-time data feeds. By extracting real-time stock data directly from Metatrader 5 and leveraging the capabilities of Google Cloud Platform, our project enables near real-time analysis of stock market data, focusing on identifying stocks with significant price variations to identify trading opportunities.

## How it Works

The project utilizes PySpark, a Python library for distributed data processing, to perform batch processing on the stock data. The data extraction process fetches real-time stock data from Metatrader 5 and stores it in a suitable storage solution, such as Google Cloud Storage or Hadoop Distributed File System (HDFS).

PySpark is then used to process the stored data in batch mode, performing calculations on minute-to-minute stock price variations and identifying the stocks with the highest positive or negative changes within the specified time window. This information is crucial for traders aiming to enter trades quickly and take advantage of potential market opportunities.

The processed results are stored in a data storage solution, such as Google BigQuery or another suitable database, allowing for easy access, querying, and visualization of the identified stocks with significant price variations. This information can be used by traders, investors, and analysts to make informed decisions regarding their stock portfolios, with a particular focus on executing trades with a good risk-reward ratio.

## Used Technologies

For this project, the following tools and technologies were utilized:

- **Metatrader 5 & MQL5**: Metatrader 5 and MQL5 were used for data gathering, providing real-time stock data directly from the trading platform.

- **Prefect & GitHub Actions**: Prefect and GitHub Actions were used for workflow orchestration, allowing for the coordination and scheduling of data processing tasks.

- **Terraform**: Terraform was chosen as the Infrastructure-as-Code (IaC) tool to provision and manage the necessary infrastructure resources on Google Cloud Platform (GCP) for the data pipeline.

- **Google Cloud Storage (GCS)**: Google Cloud Storage was used as the data lake for storing the raw data retrieved from Metatrader 5 before further processing.

- **Google BigQuery**: Google BigQuery served as the project's data warehouse, enabling scalable storage and efficient querying of the processed data.

- **Dataproc, Clusters, Jobs & Cloud Scheduler**: GCP's Dataproc, Clusters, Jobs, and Cloud Scheduler were used for the transformation of raw data into refined data. These services facilitated the distributed processing and transformation of the data using PySpark.

- **Google Looker Studio** : Google Looker studio was used for visualizations, providing a platform to create and share interactive dashboards and reports based on the processed data.

These technologies were carefully chosen to ensure efficient data processing, reliable infrastructure management, and meaningful visualizations for the project.

## Key Features

- **Real-time data extraction from the Metatrader 5 trading platform**: The project leverages Python scripts to extract real-time stock data from Metatrader 5. The extracted data is stored in files generated by Metatrader 5 and uploaded to the project's Google Cloud Storage (GCS) bucket.

- **Ingestion of data into Google BigQuery**: The processed data is ingested into Google BigQuery, serving as the project's data warehouse. The BigQuery dataset table is partitioned by date, providing optimization for future queries. Additionally, the table is clustered by Ticker, improving query performance by physically organizing similar data together.

- **Efficient data management and storage**: The project utilizes Google Cloud Storage as a data lake for storing the raw data files generated by Metatrader 5. After the data is processed and ingested into Google BigQuery, the local files are deleted to free up storage space. Additionally, a follow-up list is updated to keep track of which files have already been processed, ensuring data integrity and preventing duplication.

- **Prefect orchestration for workflow management**: Prefect is used for orchestrating the data ingestion and processing workflows. It enables the scheduling, coordination, and monitoring of the different steps involved in the data pipeline, ensuring the timely execution of tasks and the proper handling of dependencies.

- **Batch processing and transformation with PySpark in Dataproc using Clusters, Jobs & Cloud Scheduler**: PySpark is utilized to process the uploaded data in batch mode. The data transformation process is carried out using PySpark on Google Cloud Dataproc. Clusters are provisioned to execute PySpark jobs, which perform the necessary transformations on the data. Cloud Scheduler is employed to create recurring schedules for the PySpark jobs, ensuring that the data is regularly updated and refined.

These key features collectively enable the project to efficiently ingest real-time stock data, process it in a scalable and distributed manner, and store it in Google BigQuery for further analysis and decision-making. The use of BigQuery partitioning and clustering enhances query performance and optimizes data storage. The transformation process utilizing PySpark on Dataproc ensures the data is refined and up to date, while Prefect orchestration facilitates the reliable and streamlined execution of the entire data pipeline.

## Installation and Setup

To set up the project, follow the detailed installation and configuration instructions provided below. The guide will walk you through the necessary steps, including the installation of required dependencies and the configuration of GCP services.

# 1 - Metatrader 5

# 2 - GCP

- **2.1 - Setup GCP Project**
Create new project in Google Cloud Console ‚Üí switch to that newly created project

- **2.2 - Create Service Account**

# 3 - Terraform

# 4 - Ingest√£o

# 5 - DW

# 6 - Transformation - Option A - GCP, Option B - Local

# 7 - Report


## Usage

Once the project is properly installed and configured, you can run the batch processing job using PySpark to retrieve and process real-time stock data. The resulting stocks with significant price variations can be accessed and analyzed through the designated data storage solution. Detailed instructions and usage examples can be found in the [Usage Guide](link-to-usage-guide).

## Contributing

Contributions to the project are welcome! If you would like to contribute, please follow the guidelines outlined in the [Contributing Guide](link-to-contributing-guide). This guide provides information on how to set up a development environment, submit bug reports, suggest improvements, and propose new features.

## Next Steps

- Using streaming process instead of batch processing
- Just the tip of iceberg for future trading strategies
- Create the version with local processing using Docker and PostgreSQL